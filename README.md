# Transformer From Scratch (PyTorch)

A clean, modular implementation of a Transformer model built step-by-step from scratch using PyTorch.

This project focuses on deeply understanding the architecture behind the original paper:

> "Attention Is All You Need" (Vaswani et al., 2017)

---

## ğŸš€ Project Status

- âœ… Dataset pipeline  
- âœ… Tokenization (spaCy)  
- âœ… Vocabulary building  
- âœ… Numericalization  
- âœ… Padding & DataLoader  
- âœ… Token Embedding  
- âœ… Positional Encoding  
- â¬œ Multi-Head Attention  
- â¬œ Encoder & Decoder  
- â¬œ Full Transformer  
- â¬œ Training loop  

---

## ğŸ“‚ Project Structure

```
project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ embedding.py
â”‚       â””â”€â”€ positional_encoding.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_dataset.py
â”‚   â”œâ”€â”€ test_embedding.py
â”‚   â”œâ”€â”€ test_positional_encoding.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸ§  Architecture Overview (Current)

```
Raw Text
â†“
Tokenization (spaCy)
â†“
Vocabulary (torchtext)
â†“
Numericalization
â†“
Padding & Batching
â†“
Embedding Layer
â†“
Positional Encoding
â†“
(Next: Multi-Head Attention)
```

---

## ğŸ“¦ Dataset

We use the **Multi30k** German â†’ English translation dataset via `torchtext`.

Each training sample looks like:

```
("Zwei MÃ¤nner laufen.", "Two men are running.")
```

---

## ğŸ”¤ Vocabulary & Numericalization

Special tokens used:

- `<sos>` â€” Start of sentence
- `<eos>` â€” End of sentence
- `<pad>` â€” Padding
- `<unk>` â€” Unknown word

Each sentence is converted into integer indices before being fed into the model.

---

## ğŸ§© Embedding Layer

Implemented using `nn.Embedding`.

Converts:

```
[45, 89, 120]
```

into:

```
[
  [vector_128],
  [vector_128],
  [vector_128]
]
```

Each token becomes a dense representation of size `d_model`.

---

## ğŸ“ Positional Encoding

Since Transformers do not use recurrence (no LSTM / RNN), we add positional information using sinusoidal positional encoding:

PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))  
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

This allows the model to understand word order.

---

## ğŸ³ Docker Support

Build the container:

```bash
docker build -t transformer-project .
```

Run it:

```bash
docker run -it transformer-project
```

The container includes:

- PyTorch
- torchtext
- spaCy
- Language models
- All required dependencies

---

## ğŸ§ª Running Tests

We use `pytest` for modular testing.

Install pytest:

```bash
pip install pytest
```

Run all tests:

```bash
pytest
```

---

## âš™ï¸ Setup (Without Docker)

1. Create virtual environment
2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Download spaCy models:

```bash
python -m spacy download de_core_news_sm
python -m spacy download en_core_web_sm
```

---

## ğŸ¯ Goal of This Project

This is a learning-focused implementation of the Transformer architecture with:

- Clear modular code
- Full test coverage
- Clean project structure
- Production-style practices

The goal is to deeply understand each building block instead of relying entirely on `nn.Transformer`.

---

## ğŸ“š References

- Vaswani et al., 2017 â€” Attention Is All You Need
- PyTorch Documentation
- torchtext Documentation

---

## ğŸ”œ Upcoming Work

- Multi-Head Attention implementation
- Encoder block
- Decoder block
- Full Transformer stack
- Training loop
- BLEU evaluation

---

## ğŸ‘©â€ğŸ’» Author

Built as a step-by-step exploration of Transformer architecture in PyTorch.